{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSeo4XItT27"
      },
      "source": [
        "### **TransLOB** \n",
        "This is the implementation of the model TransLOB proposed in the paper *Transformers for limit order books by james wallbridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUo7jxtezAoE",
        "outputId": "153e54a9-9275-4f20-d912-fc5af2b4133f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# load packages\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from keras import layers\n",
        "from keras.layers import Conv1D, BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2-jdRkKvSTP"
      },
      "source": [
        "### **Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "OcSo3GDzz-qa",
        "outputId": "fcdd62a1-fddb-4ea4-f31e-262af38d60c3"
      },
      "outputs": [],
      "source": [
        "# please change the data_path to your local path and unzip the file\n",
        "\n",
        "dec_data = np.loadtxt('/published/BenchmarkDatasets/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt')\n",
        "dec_train = dec_data[:, :int(dec_data.shape[1] * 0.8)]\n",
        "dec_val = dec_data[:, int(dec_data.shape[1] * 0.8):]\n",
        "\n",
        "dec_test1 = np.loadtxt('/published/BenchmarkDatasets/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt')\n",
        "dec_test2 = np.loadtxt('/published/BenchmarkDatasets/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt')\n",
        "dec_test3 = np.loadtxt('/published/BenchmarkDatasets/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt')\n",
        "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
        "\n",
        "\n",
        "horizon = 5         #if horizon = 5, than k = 10\n",
        "\n",
        "y_train = dec_train[-horizon, :].flatten()\n",
        "\n",
        "y_val = dec_val[-horizon, :].flatten()\n",
        "y_test = dec_test[-horizon, :].flatten()\n",
        "\n",
        "y_train = y_train[99:] - 1\n",
        "y_val = y_val[99:] - 1\n",
        "y_test = y_test[99:] - 1 \n",
        "\n",
        "dec_train = dec_train[:40, :].T\n",
        "dec_val = dec_val[:40, :].T\n",
        "dec_test = dec_test[:40, :].T\n",
        "\n",
        "print(dec_train.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x7PAu1LySOZ",
        "outputId": "5f9bad48-cffd-443e-acec-08b13b41dfa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50751\n",
            "139388\n",
            "203601\n"
          ]
        }
      ],
      "source": [
        "#Create the dataset\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
        "    def __init__(self, x, y, num_classes, n, dim):\n",
        "        \"\"\"Initialization\"\"\" \n",
        "        self.num_classes = num_classes\n",
        "        self.dim = dim\n",
        "        self.x = x   \n",
        "        self.y = y\n",
        "        self.n = n\n",
        "\n",
        "        self.length = x.shape[0] - T -self.dim + 1\n",
        "        print(self.length)\n",
        "\n",
        "        x = torch.from_numpy(x)\n",
        "        self.x = torch.unsqueeze(x, 1)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the total number of samples\"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        input = self.x[i:i+self.dim, :]\n",
        "        input = input.permute(1, 0, 2)\n",
        "        input = np.squeeze(input)\n",
        "        input = input.permute(1, 0)\n",
        "        return input, self.y[i]\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "T = 100   #horizon    \n",
        "lr = 0.0001\n",
        "num_classes = 3\n",
        "dim = 100\n",
        "n = 3 \n",
        "\n",
        "\n",
        "dataset_val = Dataset(dec_val, y_val, num_classes, n, dim)\n",
        "dataset_test = Dataset(dec_test, y_test, num_classes, n, dim)\n",
        "dataset_train = Dataset(dec_train, y_train, num_classes, n, dim)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0wAgGUv8Yhi"
      },
      "source": [
        "### **Model Architecture**\n",
        "The model architecture is specified in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mBLoNDNIAAs9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def positional_encoding(x):\n",
        "  n_levels = 100\n",
        "  pos = np.arange(0, n_levels,  1, dtype=np.float32) / (n_levels-1)\n",
        "  pos = (pos + pos) - 1\n",
        "  #pos = np.reshape(pos, (pos.shape[0]))\n",
        "  pos_final = np.zeros((x.shape[0], n_levels, 1), dtype=np.float32)\n",
        "  for i in range(pos_final.shape[0]):\n",
        "    for j in range(pos_final.shape[1]):\n",
        "      pos_final[i, j, 0] = pos[j]\n",
        "\n",
        "  pos_final = torch.tensor(pos_final).to(device)\n",
        "  x = torch.cat((x, pos_final), 2)\n",
        "  \n",
        "  return x\n",
        "\n",
        "class transLOB(nn.Module):\n",
        "    def __init__(self, in_c, out_c, seq_len, n_attlayers, n_heads, dim_linear, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "        Args:\n",
        "          in_c: the number of input channels for the first Conv1d layer in the CNN\n",
        "          out_c: the number of output channels for all Conv1d layers in the CNN\n",
        "          seq_len: the sequence length of the input data\n",
        "          n_attlayers: the number of attention layers in the transformer encoder\n",
        "          n_heads: the number of attention heads in the transformer encoder\n",
        "          dim_linear: the number of neurons in the first linear layer (fc1)\n",
        "          dim_feedforward: the number of neurons in the feed-forward layer of the transformer encoder layer\n",
        "          dropout: the dropout rate for the Dropout layer\n",
        "        '''\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_c, out_channels=out_c, kernel_size=2, stride=1, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=out_c, out_channels=out_c, kernel_size=2, dilation=2, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=out_c, out_channels=out_c, kernel_size=2, dilation=4, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=out_c, out_channels=out_c, kernel_size=2, dilation=8, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=out_c, out_channels=out_c, kernel_size=2, dilation=16, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        d_model = out_c + 1\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=0.0, batch_first=True, device=device)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm([seq_len, out_c])\n",
        "        \n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, n_attlayers)\n",
        "        \n",
        "        self.fc1 = nn.Linear(seq_len*d_model, dim_linear)\n",
        "        self.fc2 = nn.Linear(dim_linear, 3)\n",
        "     \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Pass the input tensor through a series of convolutional layers\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        # Permute the dimensions of the output from the convolutional layers so that the second dimension becomes the first\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Normalize the output from the convolutional layers\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Apply positional encoding to the output from the layer normalization\n",
        "        x = positional_encoding(x)\n",
        "\n",
        "        # Pass the output from the previous steps through the transformer encoder\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Reshape the output from the transformer encoder to have only two dimensions\n",
        "        x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
        "\n",
        "        # Apply dropout and activation function to the output from the previous step, then pass it through the first linear layer\n",
        "        x = self.dropout(self.activation(self.fc1(x)))\n",
        "\n",
        "        # Pass the output from the previous step through the second linear layer\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        # Apply softmax activation to the output from the second linear layer\n",
        "        forecast_y = torch.softmax(x, dim=1)\n",
        "        \n",
        "        return forecast_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CaGko8X8lmC"
      },
      "source": [
        "### **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s_u5esKfTT-S"
      },
      "outputs": [],
      "source": [
        "model = transLOB(in_c=40, out_c=14, seq_len=dim, n_attlayers=2, n_heads=3, dim_linear=64, dim_feedforward=60, dropout=0.1)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=1e-5)\n",
        "\n",
        "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
        "    \n",
        "    train_losses = np.zeros(epochs)\n",
        "    test_losses = np.zeros(epochs)\n",
        "    best_test_loss = np.inf\n",
        "    best_test_epoch = 0\n",
        "\n",
        "    for it in tqdm(range(epochs)):\n",
        "        \n",
        "        model.train()\n",
        "        t0 = datetime.now()\n",
        "        train_loss = []\n",
        "        for inputs, targets in train_loader:\n",
        "            # move data to GPU\n",
        "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
        "            # print(\"inputs.shape:\", inputs.shape)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            # print(\"about to get model output\")\n",
        "            outputs = model(inputs)\n",
        "            # print(\"done getting model output\")\n",
        "            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n",
        "            loss = criterion(outputs, targets)\n",
        "            # Backward and optimize\n",
        "            # print(\"about to optimize\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "        # Get train loss and test loss\n",
        "        train_loss = np.mean(train_loss) # a little misleading\n",
        "    \n",
        "        model.eval()\n",
        "        test_loss = []\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss.append(loss.item())\n",
        "        test_loss = np.mean(test_loss)\n",
        "\n",
        "        # Save losses\n",
        "        train_losses[it] = train_loss\n",
        "        test_losses[it] = test_loss\n",
        "        \n",
        "        if test_loss < best_test_loss:\n",
        "            torch.save(model, '/best_model_transformer')\n",
        "            best_test_loss = test_loss\n",
        "            best_test_epoch = it\n",
        "            print('model saved')\n",
        "\n",
        "        dt = datetime.now() - t0\n",
        "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
        "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
        "\n",
        "    return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "EwRTC160vqW5",
        "outputId": "cfb7c29c-cac4-4311-c2b0-f8fd12407728"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
        "                                    train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja-2FWDj8pt6"
      },
      "source": [
        "### **Model Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "TFg5d6CzTgWS",
        "outputId": "4c8d280a-8140-4b20-a6fc-1ee247030b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8a6e3c677951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy_score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_prediction' is not defined"
          ]
        }
      ],
      "source": [
        "model = torch.load('/best_model_transformer')\n",
        "\n",
        "n_correct = 0.\n",
        "n_total = 0.\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "for inputs, targets in test_loader:\n",
        "    # Move to GPU\n",
        "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Get prediction\n",
        "    # torch.max returns both max and argmax\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # update counts\n",
        "    n_correct += (predictions == targets).sum().item()\n",
        "    n_total += targets.shape[0]\n",
        "\n",
        "    all_targets.append(targets.cpu().numpy())\n",
        "    all_predictions.append(predictions.cpu().numpy())\n",
        "\n",
        "test_acc = n_correct / n_total\n",
        "print(f\"Test acc: {test_acc:.4f}\")\n",
        "\n",
        "all_targets = np.concatenate(all_targets)    \n",
        "all_predictions = np.concatenate(all_predictions)   \n",
        "\n",
        "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
        "print(classification_report(all_targets, all_predictions, digits=4))\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "c = confusion_matrix(all_targets, all_predictions, normalize=\"true\")\n",
        "disp = ConfusionMatrixDisplay(c)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
